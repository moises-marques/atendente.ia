# main.py
import os
import json
import random
from fastapi import FastAPI, Header
from pydantic import BaseModel
from dotenv import load_dotenv
import difflib
from datetime import datetime
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="Atendente IA - RoboBot (Modo H√≠brido)")

origins = [
    "https://meu-robochat.netlify.app",  # URL do seu frontend
    "http://localhost:3000",  # se quiser testar localmente
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # quem pode acessar
    allow_credentials=True,
    allow_methods=["*"],    # GET, POST, etc
    allow_headers=["*"],    # headers
)


load_dotenv()

# ==========================
# 1) Carrega a chave de API e mostra mensagem
# ==========================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    print("ERRO: Vari√°vel OPENAI_API_KEY N√ÉO est√° definida ou o arquivo .env n√£o foi lido.")
    print("Verifique se o arquivo .env est√° na pasta raiz e se o nome da vari√°vel est√° correto.")
else:
    print(f"SUCESSO: Chave de API carregada. In√≠cio: {OPENAI_API_KEY[:5]}... Fim: {OPENAI_API_KEY[-5:]}")

# CONFIG
USE_MOCK = os.getenv("USE_MOCK", "true").lower() in ("1", "true", "yes")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # continuar√° √∫til mais tarde
KB_FILE = "kb.json"  # arquivo de FAQ/knowlege base edit√°vel pelo cliente
LOG_FILE = "conversations.log"

# app = FastAPI(title="Atendente IA - RoboBot (Modo H√≠brido)")

# Modelos de request/response
class MessageIn(BaseModel):
    message: str
    user_id: str | None = None  # opcional, √∫til pra logs/personaliza√ß√£o

class MessageOut(BaseModel):
    reply: str
    meta: dict | None = None

# ---------- Utilities ----------
def log_message(user_id, user_msg, bot_reply, mode):
    entry = {
        "ts": datetime.utcnow().isoformat()+"Z",
        "user_id": user_id,
        "message": user_msg,
        "reply": bot_reply,
        "mode": mode
    }
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def load_kb():
    if not os.path.exists(KB_FILE):
        # cria um KB m√≠nimo de exemplo se n√£o existir
        demo = [
            {"q":"hor√°rio de atendimento", "a":"Nosso hor√°rio √© Segunda a Sexta das 9h √†s 18h."},
            {"q":"pre√ßo do plano b√°sico", "a":"O plano b√°sico custa R$ 39/m√™s. Temos descontos para annual."},
            {"q":"como cancelar", "a":"Para cancelar, acesse sua conta > Configura√ß√µes > Cancelar assinatura."}
        ]
        with open(KB_FILE, "w", encoding="utf-8") as f:
            json.dump(demo, f, ensure_ascii=False, indent=2)
        return demo
    with open(KB_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

KB = load_kb()

def kb_lookup(question, top_n=1, cutoff=0.5):
    # busca por similaridade simples (difflib)
    questions = [entry["q"] for entry in KB]
    matches = difflib.get_close_matches(question.lower(), questions, n=top_n, cutoff=cutoff)
    if matches:
        # devolve a primeira correspondente e a resposta
        matched_q = matches[0]
        for entry in KB:
            if entry["q"] == matched_q:
                return entry["a"], matched_q
    return None, None

# ---------- Mock response logic ----------
def mock_reply(user_msg: str):
    txt = user_msg.strip().lower()

    # 1) Teste r√°pido
    if txt == "teste":
        return "üß© Modo teste: o servidor est√° funcionando perfeitamente!", {"type":"test"}

    # 2) Inten√ß√µes de a√ß√£o simuladas (criando ticket, verificando pedido, etc)
    if any(w in txt for w in ("abrir chamado","criar ticket","abrir ticket","abrir chamado")):
        ticket_id = f"T-{random.randint(1000,9999)}"
        reply = f"‚úÖ Chamado criado com sucesso. ID: {ticket_id}. Nosso time responder√° em at√© 24h."
        return reply, {"type":"action", "action":"create_ticket", "ticket_id":ticket_id}

    if "status do pedido" in txt or ("status" in txt and "pedido" in txt):
        # resposta simulada
        reply = "O pedido #1234 est√° em transporte e deve chegar em 3 dias √∫teis."
        return reply, {"type":"action","action":"check_order","order_id":"1234"}

    # 3) Sauda√ß√£o e small talk
    if any(g in txt for g in ("ol√°","oi","bom dia","boa tarde","boa noite","fala")):
        return "Ol√°! Eu sou o Rob√¥Bot. Como posso ajudar voc√™ hoje?", {"type":"small_talk"}

    # 4) Busca na KB (FAQ)
    kb_answer, matched_q = kb_lookup(txt, top_n=1, cutoff=0.5)
    if kb_answer:
        return kb_answer, {"type":"kb", "matched_q": matched_q}

    # 5) Regras simples por palavras-chave
    if "pre√ßo" in txt or "valor" in txt or "custo" in txt:
        return "Temos planos que come√ßam em R$ 39/m√™s. Quer que eu envie os detalhes por e-mail?", {"type":"pricing"}

    # 6) Fallback "inteligente" do mock (mais natural)
    fallback_variants = [
        "Desculpe ‚Äî n√£o entendi completamente. Pode dizer de outro jeito?",
        "Posso ajudar com: 'abrir chamado', 'pr√©√ßos', 'hor√°rio de atendimento' ou 'status do pedido'.",
        "Ainda n√£o sei isso, quer que eu crie um chamado para a equipe humana?"
    ]
    return random.choice(fallback_variants), {"type":"fallback"}

# ---------- Endpoint ----------
@app.post("/chat", response_model=MessageOut)
async def chat_endpoint(body: MessageIn):
    x_openai_key: str | None = Header(None)  # <-- aqui voc√™ declara o header
    user_msg = body.message
    user_id = body.user_id or "anon"

     # ===============================
    # NOVO: verifica se veio chave do front-end
    # ===============================
    effective_api_key = x_openai_key or OPENAI_API_KEY  # se cliente passar chave, usa ela

    if effective_api_key and not USE_MOCK:
        # aqui entra a chamada real para OpenAI usando a chave do cliente
        # por enquanto, s√≥ placeholder
        reply = f"‚úÖ Modo inteligente ativado! Chave recebida: {effective_api_key[:5]}... (n√£o mostrada toda por seguran√ßa)"
        log_message(user_id, user_msg, reply, mode="openai")
        return {"reply": reply, "meta": {"mode":"openai"}}


    if USE_MOCK:
        reply, meta = mock_reply(user_msg)
        log_message(user_id, user_msg, reply, mode="mock")
        return {"reply": reply, "meta": meta}

    # caso n√£o esteja em mock, aqui voc√™ colocaria a chamada real √† OpenAI / RAG
    # por enquanto, um fallback claro
    reply = "‚ö†Ô∏è Modo real n√£o habilitado. Configure USE_MOCK=false e a integra√ß√£o com LLM."
    log_message(user_id, user_msg, reply, mode="disabled")
    return {"reply": reply, "meta": {"mode":"disabled"}}

